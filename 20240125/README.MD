# AIFFEL Campus Online 7th Code Peer Review Templete

- ì½”ë” : ì˜¤ìš°ì§„
- ë¦¬ë·°ì–´ : ê¹€ì–‘í¬

ğŸ”‘ **PRT(Peer Review Template)**

- [ã…‡]  **1. ì£¼ì–´ì§„ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ì™„ì„±ëœ ì½”ë“œê°€ ì œì¶œë˜ì—ˆë‚˜ìš”?**
    - ë¬¸ì œì—ì„œ ìš”êµ¬í•˜ëŠ” ìµœì¢… ê²°ê³¼ë¬¼ì´ ì²¨ë¶€ë˜ì—ˆëŠ”ì§€ í™•ì¸
    - ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ì™„ì„±ëœ ì½”ë“œë€ í”„ë¡œì íŠ¸ ë£¨ë¸Œë¦­ 3ê°œ ì¤‘ 2ê°œ, 
    í€˜ìŠ¤íŠ¸ ë¬¸ì œ ìš”êµ¬ì¡°ê±´ ë“±ì„ ì§€ì¹­
        - í•´ë‹¹ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ë¶€ë¶„ì˜ ì½”ë“œ ë° ê²°ê³¼ë¬¼ì„ ê·¼ê±°ë¡œ ì²¨ë¶€

[ë¦¬ë·°]      
- ë£¨ë¸Œë¦­1 : ê³µë°±ê³¼ íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬, í† í¬ë‚˜ì´ì§•, ë³‘ë ¬ë°ì´í„° êµ¬ì¶•ì˜ ê³¼ì •ì´ ì ì ˆíˆ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.    
```
# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
data_path = '~/aiffel/transformer_chatbot/data/ChatbotData .csv'  # ë°ì´í„° ê²½ë¡œ ì„¤ì •
chatbot_data = pd.read_csv(data_path)  # pandasë¥¼ ì‚¬ìš©í•˜ì—¬ CSV íŒŒì¼ ë¡œë“œ

# ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_sentence(sentence):
    sentence = sentence.strip()  # ì–‘ìª½ ê³µë°± ì œê±°
    sentence = re.sub(r"([?.!,])", r" \1 ", sentence)  # êµ¬ë‘ì  ë¶„ë¦¬
    sentence = re.sub(r'[" "]+', " ", sentence)  # ì—¬ë¶„ì˜ ê³µë°± ì œê±°
    sentence = re.sub(r"[^ê°€-í£?.!,]+", " ", sentence)  # í•œê¸€, êµ¬ë‘ì ë§Œ ë‚¨ê¹€
    return sentence

# ì§ˆë¬¸ê³¼ ë‹µë³€ ì»¬ëŸ¼ ì „ì²˜ë¦¬
questions = [preprocess_sentence(sentence) for sentence in chatbot_data['Q']]
answers = [preprocess_sentence(sentence) for sentence in chatbot_data['A']]
# SubwordTextEncoderë¡œ í† í°í™”
tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)

# ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í° ì •ì˜
START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]
```

- ë£¨ë¸Œë¦­2 : êµ¬í˜„í•œ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì´ í•œêµ­ì–´ ë³‘ë ¬ ë°ì´í„° í•™ìŠµ ì‹œ ì•ˆì •ì ìœ¼ë¡œ ìˆ˜ë ´í•˜ì˜€ìŠµë‹ˆë‹¤.  
    
- [ã…‡]  **2. ì „ì²´ ì½”ë“œì—ì„œ ê°€ì¥ í•µì‹¬ì ì´ê±°ë‚˜ ê°€ì¥ ë³µì¡í•˜ê³  ì´í•´í•˜ê¸° ì–´ë ¤ìš´ ë¶€ë¶„ì— ì‘ì„±ëœ 
ì£¼ì„ ë˜ëŠ” doc stringì„ ë³´ê³  í•´ë‹¹ ì½”ë“œê°€ ì˜ ì´í•´ë˜ì—ˆë‚˜ìš”?**
    - í•´ë‹¹ ì½”ë“œ ë¸”ëŸ­ì— doc string/annotationì´ ë‹¬ë ¤ ìˆëŠ”ì§€ í™•ì¸
    - í•´ë‹¹ ì½”ë“œê°€ ë¬´ìŠ¨ ê¸°ëŠ¥ì„ í•˜ëŠ”ì§€, ì™œ ê·¸ë ‡ê²Œ ì§œì—¬ì§„ê±´ì§€, ì‘ë™ ë©”ì»¤ë‹ˆì¦˜ì´ ë­”ì§€ ê¸°ìˆ .
    - ì£¼ì„ì„ ë³´ê³  ì½”ë“œ ì´í•´ê°€ ì˜ ë˜ì—ˆëŠ”ì§€ í™•ì¸
        - ì˜ ì‘ì„±ë˜ì—ˆë‹¤ê³  ìƒê°ë˜ëŠ” ë¶€ë¶„ì„ ê·¼ê±°ë¡œ ì²¨ë¶€í•©ë‹ˆë‹¤.


[ë¦¬ë·°]  
```
class MultiHeadAttention(tf.keras.layers.Layer):

  def __init__(self, d_model, num_heads, name="multi_head_attention"):
    super(MultiHeadAttention, self).__init__(name=name)
    self.num_heads = num_heads
    self.d_model = d_model

    assert d_model % self.num_heads == 0

    self.depth = d_model // self.num_heads

    self.query_dense = tf.keras.layers.Dense(units=d_model)
    self.key_dense = tf.keras.layers.Dense(units=d_model)
    self.value_dense = tf.keras.layers.Dense(units=d_model)

    self.dense = tf.keras.layers.Dense(units=d_model)

  def split_heads(self, inputs, batch_size):
    inputs = tf.reshape(
        inputs, shape=(batch_size, -1, self.num_heads, self.depth))
    return tf.transpose(inputs, perm=[0, 2, 1, 3])

  def call(self, inputs):
    query, key, value, mask = inputs['query'], inputs['key'], inputs[
        'value'], inputs['mask']
    batch_size = tf.shape(query)[0]

    # Q, K, Vì— ê°ê° Denseë¥¼ ì ìš©í•©ë‹ˆë‹¤
    query = self.query_dense(query)  # Queryì— Dense ì ìš©
    key = self.key_dense(key)        # Keyì— Dense ì ìš©
    value = self.value_dense(value)  # Valueì— Dense ì ìš©

    # ë³‘ë ¬ ì—°ì‚°ì„ ìœ„í•œ ë¨¸ë¦¬ë¥¼ ì—¬ëŸ¬ ê°œ ë§Œë“­ë‹ˆë‹¤
    query = self.split_heads(query, batch_size)
    key = self.split_heads(key, batch_size)
    value = self.split_heads(value, batch_size)

    # ìŠ¤ì¼€ì¼ë“œ ë‹· í”„ë¡œë•íŠ¸ ì–´í…ì…˜ í•¨ìˆ˜
    scaled_attention = scaled_dot_product_attention(query, key, value, mask)

    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])

    # ì–´í…ì…˜ ì—°ì‚° í›„ì— ê° ê²°ê³¼ë¥¼ ë‹¤ì‹œ ì—°ê²°(concatenate)í•©ë‹ˆë‹¤
    concat_attention = tf.reshape(scaled_attention,
                                  (batch_size, -1, self.d_model))

    # ìµœì¢… ê²°ê³¼ì—ë„ Denseë¥¼ í•œ ë²ˆ ë” ì ìš©í•©ë‹ˆë‹¤
    outputs = self.dense(concat_attention)

    return outputs
```


        
- [ã…‡]  **3. ì—ëŸ¬ê°€ ë‚œ ë¶€ë¶„ì„ ë””ë²„ê¹…í•˜ì—¬ ë¬¸ì œë¥¼ â€œí•´ê²°í•œ ê¸°ë¡ì„ ë‚¨ê²¼ê±°ë‚˜â€ 
â€ìƒˆë¡œìš´ ì‹œë„ ë˜ëŠ” ì¶”ê°€ ì‹¤í—˜ì„ ìˆ˜í–‰â€í•´ë´¤ë‚˜ìš”?**
    - ë¬¸ì œ ì›ì¸ ë° í•´ê²° ê³¼ì •ì„ ì˜ ê¸°ë¡í•˜ì˜€ëŠ”ì§€ í™•ì¸ ë˜ëŠ”
    - ë¬¸ì œì—ì„œ ìš”êµ¬í•˜ëŠ” ì¡°ê±´ì— ë”í•´ ì¶”ê°€ì ìœ¼ë¡œ ìˆ˜í–‰í•œ ë‚˜ë§Œì˜ ì‹œë„, 
    ì‹¤í—˜ì´ ê¸°ë¡ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        - ì˜ ì‘ì„±ë˜ì—ˆë‹¤ê³  ìƒê°ë˜ëŠ” ë¶€ë¶„ì„ ê·¼ê±°ë¡œ ì²¨ë¶€í•©ë‹ˆë‹¤.


[ë¦¬ë·°] ì—ëŸ¬ ì—†ì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ì˜€ìŠµë‹ˆë‹¤.  

        
- [ã…‡]  **4. íšŒê³ ë¥¼ ì˜ ì‘ì„±í–ˆë‚˜ìš”?** (2023.11.07 - ìƒëµ)
    - ì£¼ì–´ì§„ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ì™„ì„±ëœ ì½”ë“œ ë‚´ì§€ í”„ë¡œì íŠ¸ ê²°ê³¼ë¬¼ì— ëŒ€í•´
    ë°°ìš´ì ê³¼ ì•„ì‰¬ìš´ì , ëŠë‚€ì  ë“±ì´ ìƒì„¸íˆ ê¸°ë¡ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        - ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ê²½ìš°,
        ì¸í’‹ì´ ë“¤ì–´ê°€ ìµœì¢…ì ìœ¼ë¡œ ì•„ì›ƒí’‹ì´ ë‚˜ì˜¤ê¸°ê¹Œì§€ì˜ ì „ì²´ íë¦„ì„ ë„ì‹í™”í•˜ì—¬ 
        ëª¨ë¸ ì•„í‚¤í…ì³ì— ëŒ€í•œ ì´í•´ë¥¼ ë•ê³  ìˆëŠ”ì§€ í™•ì¸

[ë¦¬ë·°] ì´ë²ˆ í”„ë¡œì íŠ¸ì—ì„œ í•™ìŠµí•œ ë‚´ìš©ì„ ì¶”í›„ ì–´ë–»ê²Œ ì ìš©í• ì§€ ì‘ì„±í•´ì£¼ì…¨ìŠµë‹ˆë‹¤.  

ì´ í”„ë¡œì íŠ¸ëŠ” ë”¥ëŸ¬ë‹ê³¼ NLPì˜ ì‹¤ì œ ì ìš© ì‚¬ë¡€ë¥¼ ê²½í—˜í•  ìˆ˜ ìˆëŠ” ê¸°íšŒì˜€ìœ¼ë©°. ì½”ë“œë¥¼ ì‘ì„±í•˜ë©° ì–»ì€ ì§€ì‹ê³¼ ê²½í—˜ì€ í–¥í›„ ìœ ì‚¬í•œ í”„ë¡œì íŠ¸ë‚˜ ë‹¤ë¥¸ ê¸°ê³„ í•™ìŠµ ì‘ì—…ì— í° ë„ì›€ì´ ë  ê²ƒê°™ìŠµë‹ˆë‹¤. ë°ì´í„° ì „ì²˜ë¦¬ë¶€í„° ëª¨ë¸ êµ¬ì¶•, ìµœì í™”, í‰ê°€ì— ì´ë¥´ê¸°ê¹Œì§€ ë‹¤ì–‘í•œ ë‹¨ê³„ì—ì„œ ì¤‘ìš”í•œ êµí›ˆì„ ì–»ì—ˆìŠµë‹ˆë‹¤.  



- [ã…‡]  **5. ì½”ë“œê°€ ê°„ê²°í•˜ê³  íš¨ìœ¨ì ì¸ê°€ìš”?**
    - íŒŒì´ì¬ ìŠ¤íƒ€ì¼ ê°€ì´ë“œ (PEP8) ë¥¼ ì¤€ìˆ˜í•˜ì˜€ëŠ”ì§€ í™•ì¸
    - ì½”ë“œ ì¤‘ë³µì„ ìµœì†Œí™”í•˜ê³  ë²”ìš©ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ëª¨ë“ˆí™”(í•¨ìˆ˜í™”) í–ˆëŠ”ì§€
        - ì˜ ì‘ì„±ë˜ì—ˆë‹¤ê³  ìƒê°ë˜ëŠ” ë¶€ë¶„ì„ ê·¼ê±°ë¡œ ì²¨ë¶€í•©ë‹ˆë‹¤.

[ë¦¬ë·°] í´ë˜ìŠ¤ì™€ í•¨ìˆ˜ë¡œ ì˜ ì‘ì„±ë˜ì–´ìˆìŠµë‹ˆë‹¤.   


```
def decoder_inference(sentence):
  sentence = preprocess_sentence(sentence)

  # ì…ë ¥ëœ ë¬¸ì¥ì„ ì •ìˆ˜ ì¸ì½”ë”© í›„, ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ì•ë’¤ë¡œ ì¶”ê°€.
  # ex) Where have you been? â†’ [[8331   86   30    5 1059    7 8332]]
  sentence = tf.expand_dims(
      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)

  # ë””ì½”ë”ì˜ í˜„ì¬ê¹Œì§€ì˜ ì˜ˆì¸¡í•œ ì¶œë ¥ ì‹œí€€ìŠ¤ê°€ ì§€ì†ì ìœ¼ë¡œ ì €ì¥ë˜ëŠ” ë³€ìˆ˜.
  # ì²˜ìŒì—ëŠ” ì˜ˆì¸¡í•œ ë‚´ìš©ì´ ì—†ìŒìœ¼ë¡œ ì‹œì‘ í† í°ë§Œ ë³„ë„ ì €ì¥. ex) 8331
  output_sequence = tf.expand_dims(START_TOKEN, 0)

  # ë””ì½”ë”ì˜ ì¸í¼ëŸ°ìŠ¤ ë‹¨ê³„
  for i in range(MAX_LENGTH):
    # ë””ì½”ë”ëŠ” ìµœëŒ€ MAX_LENGTHì˜ ê¸¸ì´ë§Œí¼ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ì„ ë°˜ë³µí•©ë‹ˆë‹¤.
    predictions = model(inputs=[sentence, output_sequence], training=False)
    predictions = predictions[:, -1:, :]

    # í˜„ì¬ ì˜ˆì¸¡í•œ ë‹¨ì–´ì˜ ì •ìˆ˜
    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)

    # ë§Œì•½ í˜„ì¬ ì˜ˆì¸¡í•œ ë‹¨ì–´ê°€ ì¢…ë£Œ í† í°ì´ë¼ë©´ forë¬¸ì„ ì¢…ë£Œ
    if tf.equal(predicted_id, END_TOKEN[0]):
      break

    # ì˜ˆì¸¡í•œ ë‹¨ì–´ë“¤ì€ ì§€ì†ì ìœ¼ë¡œ output_sequenceì— ì¶”ê°€ë©ë‹ˆë‹¤.
    # ì´ output_sequenceëŠ” ë‹¤ì‹œ ë””ì½”ë”ì˜ ì…ë ¥ì´ ë©ë‹ˆë‹¤.
    output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)

  return tf.squeeze(output_sequence, axis=0)
```


